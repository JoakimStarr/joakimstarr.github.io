<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <title>计量｜线性回归完全指南：从理论推导到实例应用 | 文人病</title>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="Joakim Starr 文人病 我从人间走过">
  <meta name="theme-color" content="#10b981">

  <link rel="canonical" href="http://example.com/2025/cmdv0aw3j000ytgs6b7sc9s60/">

  
    <link rel="shortcut icon" href="/images/graph1.png">
  

  <meta name="description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\\(&#39;, &#39;\\)&#39;]]   } };    线性回归完全指南：从理论推导到实例应用目录 基本概念 参数推导 模型设定 假设条件 假设检验 stata代码实现 python代码实现   基本概念在统计学中，线性回归（英语：Linear Regression）是利用称为线性回归方程的最小二乘函">
<meta property="og:type" content="article">
<meta property="og:title" content="计量｜线性回归完全指南：从理论推导到实例应用">
<meta property="og:url" content="http://example.com/2025/cmdv0aw3j000ytgs6b7sc9s60/index.html">
<meta property="og:site_name" content="文人病">
<meta property="og:description" content="MathJax &#x3D; {   tex: {     inlineMath: [[&#39;$&#39;, &#39;$&#39;], [&#39;\\(&#39;, &#39;\\)&#39;]]   } };    线性回归完全指南：从理论推导到实例应用目录 基本概念 参数推导 模型设定 假设条件 假设检验 stata代码实现 python代码实现   基本概念在统计学中，线性回归（英语：Linear Regression）是利用称为线性回归方程的最小二乘函">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/cmdv0aw3j000ytgs6b7sc9s60/image/graph1.png">
<meta property="og:image" content="http://example.com/images/graph1.png">
<meta property="article:published_time" content="2025-07-30T03:09:36.000Z">
<meta property="article:modified_time" content="2025-08-03T01:42:57.227Z">
<meta property="article:author" content="Joakim Starr(文人病)">
<meta property="article:tag" content="计量 数学">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/cmdv0aw3j000ytgs6b7sc9s60/image/graph1.png">
  <meta name="generator" content="Hexo 7.3.0">
  <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2025/cmdv0aw3j000ytgs6b7sc9s60/"},"headline":"计量｜线性回归完全指南：从理论推导到实例应用","description":"MathJax = {   tex: {     inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]   } }; 线性回归完全指南：从理论推导到实例应用目录 基本概念 参数推导 模型设定 假设条件 假设检验 stata代码实现 python代码实现 基本概念在统计学中，线性回归（英语：L","datePublished":"2025-07-30T03:09:36.000Z","dateModified":"2025-08-03T01:42:57.227Z","author":{"@type":"Person","name":"Joakim Starr(文人病)"},"publisher":{"@type":"Organization","name":"文人病","logo":{"@type":"ImageObject","url":"http://example.com/images/graph1.png"}}}
</script>

  
  
<link rel="stylesheet" href="/css/main.css">

  <style>
    :root {
      --sea-color-primary: #10b981;
    }
  </style>

  
<script src="/js/theme_mode.js"></script>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="stylesheet" href="/css/prism-okaidia.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
  <header class="sea-header">
    <nav class="sea-nav-wrap">
  <div class="sea-nav-logo" title="- 我从人间走过">
    <a href="/">文人病</a>
  </div>
  <div class="sea-nav-menus">
    <div id="sea-nav-toggle">
      <svg t="1716965724278" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10878" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M950.857143 768v73.142857c0 20.004571-16.566857 36.571429-36.571429 36.571429H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571429v-73.142857c0-20.004571 16.566857-36.571429 36.571429-36.571429h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571429z m0-292.571429v73.142858c0 20.004571-16.566857 36.571429-36.571429 36.571428H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571428v-73.142858c0-20.004571 16.566857-36.571429 36.571429-36.571428h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571428z m0-292.571428v73.142857c0 20.004571-16.566857 36.571429-36.571429 36.571429H109.714286c-20.004571 0-36.571429-16.566857-36.571429-36.571429V182.857143c0-20.004571 16.566857-36.571429 36.571429-36.571429h804.571428c20.004571 0 36.571429 16.566857 36.571429 36.571429z" p-id="10879"></path></svg>
    </div>

    <div id="sea-nav-dimmer"></div>
<div class="sea-menu-wrap">
  
    <a
      class="sea-menu-link "
      
      href="/archives/"
    >
      归档
    </a>
  
    <a
      class="sea-menu-link "
      
      href="/graph/"
    >
      图片
    </a>
  

  <span class="sea-menu-sep">|</span>

  
  

  
    <a href="/search" title="Search">
  <span class="sea-menu-icon" id="sea-search-icon">
    <svg t="1725410662861" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5552" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M400.085333 151.210667c-89.059556 0-172.672 47.616-218.197333 124.273777a14.222222 14.222222 0 1 0 24.462222 14.535112c40.433778-68.067556 114.659556-110.364444 193.735111-110.364445a14.222222 14.222222 0 1 0 0-28.444444zM170.197333 336.298667a14.236444 14.236444 0 0 0-17.080889 10.609777c-4.252444 18.147556-6.656 29.397333-6.656 49.592889a14.222222 14.222222 0 1 0 28.444445 0c0-17.137778 1.92-26.083556 5.902222-43.121777a14.222222 14.222222 0 0 0-10.609778-17.080889z" fill="" p-id="5553"></path><path d="M947.384889 821.944889L717.809778 592.384a70.584889 70.584889 0 0 0-16.64-12.472889 346.254222 346.254222 0 0 0 47.36-175.089778c0-192.142222-156.302222-348.444444-348.444445-348.444444-192.128 0-348.444444 156.302222-348.444444 348.444444s156.316444 348.430222 348.444444 348.430223c68.408889 0 132.209778-19.882667 186.083556-54.058667 2.915556 4.821333 6.570667 9.472 10.951111 13.852444l229.575111 229.589334a84.849778 84.849778 0 0 0 60.359111 24.974222 84.821333 84.821333 0 0 0 60.344889-24.974222 84.778667 84.778667 0 0 0 24.974222-60.330667 84.920889 84.920889 0 0 0-24.988444-60.359111zM108.529778 404.835556c0-160.768 130.787556-291.555556 291.555555-291.555556 160.782222 0 291.555556 130.787556 291.555556 291.555556 0 160.782222-130.801778 291.541333-291.555556 291.541333-160.768 0-291.555556-130.759111-291.555555-291.541333z m798.620444 497.578666c-10.766222 10.723556-29.496889 10.723556-40.248889 0L637.340444 672.839111c-1.991111-1.976889-2.730667-3.299556-2.830222-3.299555 0.042667-0.611556 0.824889-6.471111 17.080889-22.698667 16.213333-16.256 22.072889-16.995556 22.257778-17.109333 0.369778 0.113778 1.692444 0.853333 3.740444 2.887111l229.546667 229.560889c5.390222 5.390222 8.334222 12.529778 8.334222 20.138666a28.401778 28.401778 0 0 1-8.32 20.096z" p-id="5554"></path></svg>
  </span>
</a>
  


  <span class="sea-menu-icon" id="sea-theme-dark">
    <svg t="1725413107294" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10118" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M557.553778 976.355556c-257.265778 0-466.56-207.160889-466.56-464.426667 0-253.923556 206.577778-464.284444 460.501333-464.284445h0.355556c10.766222 0 20.622222 3.953778 25.443555 13.610667 4.878222 9.756444 3.740444 20.394667-2.915555 29.027556-55.722667 72.220444-85.162667 158.108444-85.162667 249.372444 0 225.891556 183.779556 409.386667 409.671111 409.386667l5.248-0.256c10.325333-0.142222 20.977778 5.859556 25.841778 15.644444a28.302222 28.302222 0 0 1-2.915556 30.051556C837.902222 910.08 703.203556 976.355556 557.553778 976.355556zM495.274667 105.016889C299.192889 135.281778 147.882667 306.161778 147.882667 509.809778c0 225.877333 183.779556 409.656889 409.671111 409.656889 108.686222 0 210.403556-42.055111 286.577778-116.977778-231.566222-27.192889-411.804444-224.625778-411.804445-463.36 0-83.427556 21.617778-163.299556 62.947556-234.112z" fill="" p-id="10119"></path><path d="M578.830222 879.132444c-186.865778 0-345.784889-133.418667-377.841778-317.269333a14.222222 14.222222 0 1 1 28.017778-4.878222c29.681778 170.183111 176.810667 293.703111 349.824 293.703111a14.222222 14.222222 0 1 1 0 28.444444zM209.991111 531.2c-7.537778 0-13.838222-6.997333-14.193778-14.606222-0.312889-6.584889-0.483556-13.795556-0.483555-20.465778 0-7.864889 6.357333-14.492444 14.222222-14.492444s14.222222 6.229333 14.222222 14.094222c0 6.229333 0.170667 13.425778 0.455111 19.584 0.369778 7.850667-5.674667 15.886222-13.525333 15.886222h-0.696889z" fill="" p-id="10120"></path><path d="M622.350222 309.930667m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" fill="" p-id="10121"></path><path d="M787.072 188.273778m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" fill="" p-id="10122"></path><path d="M731.960889 415.303111m-25.344 0a25.344 25.344 0 1 0 50.688 0 25.344 25.344 0 1 0-50.688 0Z" p-id="10123"></path></svg>
  </span>
  <span class="sea-menu-icon" id="sea-theme-light">
    <svg t="1725410359322" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4274" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M512 768c-141.376 0-256-114.624-256-256s114.624-256 256-256 256 114.624 256 256-114.624 256-256 256z m0-85.333333a170.666667 170.666667 0 1 0 0-341.333334 170.666667 170.666667 0 0 0 0 341.333334zM469.333333 85.333333a42.666667 42.666667 0 1 1 85.333334 0v85.333334a42.666667 42.666667 0 1 1-85.333334 0V85.333333z m0 768a42.666667 42.666667 0 1 1 85.333334 0v85.333334a42.666667 42.666667 0 1 1-85.333334 0v-85.333334zM85.333333 554.666667a42.666667 42.666667 0 1 1 0-85.333334h85.333334a42.666667 42.666667 0 1 1 0 85.333334H85.333333z m768 0a42.666667 42.666667 0 1 1 0-85.333334h85.333334a42.666667 42.666667 0 1 1 0 85.333334h-85.333334zM161.834667 222.165333a42.666667 42.666667 0 0 1 60.330666-60.330666l64 64a42.666667 42.666667 0 0 1-60.330666 60.330666l-64-64z m576 576a42.666667 42.666667 0 0 1 60.330666-60.330666l64 64a42.666667 42.666667 0 0 1-60.330666 60.330666l-64-64z m-515.669334 64a42.666667 42.666667 0 0 1-60.330666-60.330666l64-64a42.666667 42.666667 0 0 1 60.330666 60.330666l-64 64z m576-576a42.666667 42.666667 0 0 1-60.330666-60.330666l64-64a42.666667 42.666667 0 0 1 60.330666 60.330666l-64 64z" p-id="4275"></path></svg>
  </span>

  <span id="sea-menu-close-icon">
    <svg t="1725435896874" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4408" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M556.8 512l265.6-265.6c12.8-12.8 12.8-32 0-44.8s-32-12.8-44.8 0L512 467.2 246.4 201.6c-12.8-12.8-32-12.8-44.8 0s-12.8 32 0 44.8l265.6 265.6-265.6 265.6c-12.8 12.8-12.8 32 0 44.8 6.4 6.4 12.8 9.6 22.4 9.6s16-3.2 22.4-9.6l265.6-265.6 265.6 265.6c6.4 6.4 16 9.6 22.4 9.6s16-3.2 22.4-9.6c12.8-12.8 12.8-32 0-44.8L556.8 512z" p-id="4409"></path></svg>
  </span>
</div>
  </div>
</nav>
  </header>
  <main id="sea-main-wrapper" data-pagefind-body>
    <article class="sea-page-card-wrapper">
  <header class="sea-article-header">
    <h1 class="sea-article-title">计量｜线性回归完全指南：从理论推导到实例应用</h1>
    
      <div class="sea-post-meta sea-post-meta__center">
        <div class="sea-post-time">
  <svg t="1716964550804" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2621" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M805.49888 981.49888l-602.3168-0.76288c-86.59456-8.192-154.56768-81.3056-154.56768-170.01472L48.6144 291.73248c0-94.1568 76.60032-170.75712 170.7776-170.75712l586.10176 0c94.1568 0 170.73152 76.60032 170.73152 170.75712L976.22528 810.7008C976.2304 904.87296 899.63008 981.49888 805.49888 981.49888L805.49888 981.49888zM219.3664 190.57152c-55.79776 0-101.20192 45.38368-101.20192 101.18144l0 518.96832c0 55.79776 45.40416 101.20704 101.20192 101.20704l586.13248 0c55.77728 0 101.16096-45.40928 101.16096-101.20704L906.65984 291.73248c0-55.79776-45.38368-101.18656-101.16096-101.18656L219.3664 190.54592 219.3664 190.57152zM698.84416 290.51904c-25.60512 0-46.38208-20.77696-46.38208-46.38208l0-158.6688c0-25.6 20.77696-46.38208 46.38208-46.38208 25.6 0 46.38208 20.78208 46.38208 46.38208L745.22624 244.1216C745.22624 269.7472 724.46976 290.51904 698.84416 290.51904L698.84416 290.51904zM315.65824 290.51904c-25.60512 0-46.38208-20.77696-46.38208-46.38208l0-158.6688c0-25.6 20.77696-46.38208 46.38208-46.38208 25.6 0 46.38208 20.78208 46.38208 46.38208L362.04032 244.1216C362.04032 269.7472 341.28896 290.51904 315.65824 290.51904L315.65824 290.51904zM534.8864 794.78784l-44.27264 0c-25.6 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.78208 46.38208 46.38208C581.26848 774.01088 560.4864 794.78784 534.8864 794.78784L534.8864 794.78784zM930.79552 452.608 121.24672 452.608c-25.60512 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.77696-46.38208 46.38208-46.38208l809.5744 0c25.6 0 46.38208 20.77696 46.38208 46.38208C977.2032 431.82592 956.42624 452.608 930.79552 452.608L930.79552 452.608zM327.92576 649.03168l-44.27264 0c-25.6 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.77696 46.38208 46.38208C374.30784 628.25472 353.52576 649.03168 327.92576 649.03168L327.92576 649.03168zM534.8864 649.03168l-44.27264 0c-25.6 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.77696 46.38208 46.38208S560.4864 649.03168 534.8864 649.03168L534.8864 649.03168zM741.27872 649.03168l-44.26752 0c-25.60512 0-46.38208-20.78208-46.38208-46.38208 0-25.60512 20.77696-46.38208 46.38208-46.38208l44.26752 0c25.60512 0 46.38208 20.77696 46.38208 46.38208C787.6608 628.25472 766.90944 649.03168 741.27872 649.03168L741.27872 649.03168zM327.92576 794.78784l-44.27264 0c-25.6 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.78208-46.38208 46.38208-46.38208l44.27264 0c25.6 0 46.38208 20.78208 46.38208 46.38208C374.30784 774.01088 353.52576 794.78784 327.92576 794.78784L327.92576 794.78784zM741.27872 794.78784l-44.26752 0c-25.60512 0-46.38208-20.77696-46.38208-46.38208 0-25.6 20.77696-46.38208 46.38208-46.38208l44.26752 0c25.60512 0 46.38208 20.78208 46.38208 46.38208C787.6608 774.01088 766.90944 794.78784 741.27872 794.78784L741.27872 794.78784z" p-id="2622"></path></svg>
  <time datetime="2025-07-30T03:09:36.000Z">2025-07-30</time>
</div>
        
  <div class="sea-post-categories">
    <svg t="1716964680422" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4550" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M810.666667 85.333333a85.333333 85.333333 0 0 1 85.333333 85.333334v152.021333c36.821333 9.493333 64 42.88 64 82.645333v405.333334a128 128 0 0 1-128 128H192a128 128 0 0 1-128-128V298.666667a85.376 85.376 0 0 1 64-82.645334V170.666667a85.333333 85.333333 0 0 1 85.333333-85.333334h597.333334zM128.149333 296.170667L128 298.666667v512a64 64 0 0 0 60.245333 63.893333L192 874.666667h640a64 64 0 0 0 63.893333-60.245334L896 810.666667V405.333333a21.333333 21.333333 0 0 0-18.837333-21.184L874.666667 384H638.165333l-122.069333-101.717333a21.333333 21.333333 0 0 0-10.688-4.736l-2.986667-0.213334H149.333333a21.333333 21.333333 0 0 0-21.184 18.837334zM535.189333 213.333333l127.978667 106.666667H832V170.666667a21.333333 21.333333 0 0 0-18.837333-21.184L810.666667 149.333333H213.333333a21.333333 21.333333 0 0 0-21.184 18.837334L192 170.666667v42.666666h343.168z" p-id="4551"></path></svg>
    <a class="category-link" href="/categories/%E8%AE%A1%E9%87%8F/">计量</a>
  </div>

        
  <div class="sea-post-tags">
    <svg t="1716964811431" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6117" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M384 977.152c-20.5312 0-39.8336-7.9872-54.3232-22.4256l-260.4032-260.4032c-14.4896-14.4896-22.4256-33.7408-22.4256-54.3232s7.9872-39.8336 22.4256-54.3232l439.6032-439.6032c24.9344-24.9344 70.2464-43.7248 105.5232-43.7248h230.4c42.3424 0 76.8 34.4576 76.8 76.8v230.4c0 35.2256-18.7904 80.5888-43.6736 105.5232l-439.6032 439.6032a76.1856 76.1856 0 0 1-54.3232 22.4256zM614.4 153.6c-21.248 0-54.272 13.6704-69.2736 28.7232l-439.6032 439.6032c-4.8128 4.8128-7.424 11.2128-7.424 18.1248s2.6624 13.312 7.424 18.0736l260.4032 260.4032c4.8128 4.8128 11.2128 7.424 18.1248 7.424s13.312-2.6624 18.1248-7.424l439.6032-439.6032c15.0016-15.0016 28.7232-48.0768 28.7232-69.3248V179.2a25.6 25.6 0 0 0-25.6-25.6h-230.4z" p-id="6118"></path><path d="M742.4 358.4c-42.3424 0-76.8-34.4576-76.8-76.8S700.0576 204.8 742.4 204.8s76.8 34.4576 76.8 76.8S784.7424 358.4 742.4 358.4z m0-102.4a25.6 25.6 0 1 0 0 51.2 25.6 25.6 0 0 0 0-51.2z" p-id="6119"></path></svg>
    <a class="tag-link" href="/tags/%E8%AE%A1%E9%87%8F-%E6%95%B0%E5%AD%A6/" rel="tag">计量 数学</a>
  </div>

      </div>
    
  </header>
  <div class="sea-doc">
    
    <div class="sea-article-content">
      <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/lightgallery.js/1.0.1/css/lightgallery.min.css" /><div class=".article-gallery"><p><script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script></p>
<p><script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script></p>
<h1 id="线性回归完全指南：从理论推导到实例应用"><a href="#线性回归完全指南：从理论推导到实例应用" class="headerlink" title="线性回归完全指南：从理论推导到实例应用"></a>线性回归完全指南：从理论推导到实例应用</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li><a href="#基本概念">基本概念</a></li>
<li><a href="#参数推导">参数推导</a></li>
<li><a href="#模型设定">模型设定</a></li>
<li><a href="#假设条件">假设条件</a></li>
<li><a href="#假设检验">假设检验</a></li>
<li><a href="#stata代码实现">stata代码实现</a></li>
<li><a href="#python代码实现">python代码实现</a></li>
</ol>
<hr>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>在统计学中，线性回归（英语：Linear Regression）是利用称为线性回归方程的最小二乘函数对一个或多个解释变量和被解释变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个解释变量的情况称为一元回归，大于一个解释变量情况的叫做多元回归（multivariable linear regression）。<sup><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8">[1]</a></sup></p>
<blockquote>
<p>注：在经济学中，<strong>自变量也被称为解释变量，因变量也称为被解释变量</strong>。<strong>同时存在多个自变量的情况下，还会将自变量分为核心解释变量和控制变量，但需要注意的一点是，这种区分是人为区分的，在估计参数时，统一将其当作自变量处理</strong>。在本文当中，会统一使用解释变量和被解释变量的称呼，需注意。</p>
</blockquote>
<p>举个例子：想象你是一位房产中介，你发现房子面积越大，价格似乎越高。你想量化这种关系，这就是线性回归要解决的问题,分析解释变量对被解释变量的变化的可以解释程度。<br>线性回归的核心思想是：用一条直线（或超平面）来描述解释变量（X）和被解释变量（Y）之间的关系。比如上述例子中，这条直线的方程可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align} 
Y = \beta_0 + \beta_1X + \varepsilon 
\end{align}</script><p>其中：</p>
<ul>
<li>$Y$：被解释变量（如房价）</li>
<li>$X$：解释变量（如面积）</li>
<li>$\beta_0$：截距项（当 X=0 时 Y 的值）</li>
<li>$\beta_1$：斜率（X 每增加 1 单位，Y 的变化量，也就是 X 对 Y 的变动的解释程度）</li>
<li>$\varepsilon$：误差项（模型无法解释的部分）<br>如下图所示，其中红色的线为回归线$ Y = \beta_0 + \beta_1X + \varepsilon$，蓝色的点为真实值。<!-- <a href="image/graph1.png" title="线性回归图" class="gallery-item"><img src="image/graph1.png" style="display: block; margin: 0 auto;" alt="线性回归图" width="600"></a>
<div style="text-align: center;">图 1 线性回归图</div> -->
<!-- ![线性回归图](https://picx.zhimg.com/80/v2-d9848272ef2a336dfd1ae8ea0728589f_1440w.webp?source=d16d100b) -->
<a href="/images/graph1.png" title="线性回归图" class="gallery-item"><img src="/images/graph1.png" alt="线性回归图"></a></li>
</ul>
<blockquote>
<p>注：其实从这个地方也能看得出来，线性回归本身描述是真实值$Y$的趋势，也可以说是$Y$的均值的预测，并不是对每个真实值的准确预测，这并非线性回归的目的。即给定$X_i$的条件下，$Y_i$的均值是多少，即$E(Y_i | X_i)=Y$。线性回归这种特性是为了可解释性而存在的。例如各种深度学习模型，比如CNN、LSTM等，他们的预测能力可能非常强，但是可解释性并不那么高，这种模型还是个黑匣子，对每个变量的参数是不可控的。而线性回归模型不一样，只要模型形式确定了，数据确定了，那无论什么时候，模型的参数都是确定的，同时也是可以直接量化的。</p>
</blockquote>
<p>若你认为价格不仅可以被面积影响，还有其他因素，那么可以通过添加其他解释变量来提高回归模型的预测效果，如当地的经济发展水平，地区人口密度等，那你的模型就可以表示为：</p>
<script type="math/tex; mode=display">\begin{align} 
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \varepsilon \end{align}</script><p>  其中：</p>
<ul>
<li>$Y$：为被解释变量：房价</li>
<li>$X_1$：解释变量 1：面积</li>
<li>$X_2$：解释变量 2：本地经济发展水平</li>
<li>$X_3$：解释变量 3：地区人口密度</li>
<li>$\beta_0$：截距项（当 X=0 时 Y 的值）</li>
<li>$\beta_i$：斜率（$X_i$每增加 1 单位，Y 的变化量）</li>
<li>$\varepsilon$：误差项（模型无法解释的部分）</li>
</ul>
<p>其中$(1)$和$(2)$分别为一元线性回归和多元线性回归。</p>
<h3 id="模型设定"><a href="#模型设定" class="headerlink" title="模型设定"></a>模型设定</h3><p>更一般的，线性回归被表示为：</p>
<script type="math/tex; mode=display">\begin{align} 
Y=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_nX_n+\varepsilon \end{align}</script><p>若令：</p>
<script type="math/tex; mode=display">
\mathbf
Y =
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}，
X =
\begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1n} \\
1 & x_{21}  & x_{22} & \cdots & x_{2n} \\
\vdots &\vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{nn}
\end{bmatrix}，
\beta =
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
% \beta_2 \\
\vdots \\
\beta_n
\end{bmatrix}</script><p>则线性回归就可以被简洁的表示为：</p>
<script type="math/tex; mode=display">\begin{align} \bf Y=X\beta+\varepsilon \end{align}</script><h3 id="参数推导"><a href="#参数推导" class="headerlink" title="参数推导"></a>参数推导</h3><!-- 为照顾部分读者可能对代数推到不感兴趣，因此将推导过程省略，若需要查看详细推导过程的，可以点击下面按钮展开。 -->
<!-- <details>
<summary><b>点击展开推导过程</b></summary> -->
<p>不少人会对线性回归有所疑惑，模型数学表达式知道了，那么如何推导出参数呢？其实参数的推导是来自于数据本身的，也就是说，参数是根据数据所决定的，而不是模型所决定的。更通俗点就是由$Y、X$所决定的，于是下面让我们来推导一下参数的基本表达式，以一元线性回归为例，假设真实模型为：</p>
<script type="math/tex; mode=display">\begin{align}  Y_i=\beta_0+\beta_1X_i \end{align}</script><p>估计模型为：</p>
<script type="math/tex; mode=display">\begin{align}  \hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X_i+\varepsilon_i \end{align}</script><p>其中$\varepsilon$为误差项，$X$为解释变量，$Y$为被解释变量，$\hat{Y_i}$为模型预测的被解释变量值，在数学中，通常将$\hat{\phantom{a}}$来表示一个变量的估计值或预测值。<br>又:</p>
<script type="math/tex; mode=display">\begin{align}  \varepsilon_i = Y_i-\hat{Y_i}\end{align}</script><p>拟合的目标就是想要让，预测值$\hat{Y_i}$与实际值$Y_i$尽可能小，</p>
<script type="math/tex; mode=display">\begin{align}  \sum_{i=1}^n \varepsilon_i^2 = \sum_{i=1}^n(Y_i-\hat{Y_i})^2 \end{align}</script><!-- ![线性模型拟合数据](https://zh.d2l.ai/_images/fit-linreg.svg) -->
<p>得到最小二乘目标函数：</p>
<script type="math/tex; mode=display">\begin{align}  \min \sum_{i=1}^n (Y_i - \beta_0 - \beta_1X_i)^2 ＝ \min \sum_{i=1}^n (Y*i - \hat{Y_i})^2 ＝ min\sum_{i=1}^n \varepsilon_i^2 \end{align}</script><p>对$\beta_0, \beta_1$分别求偏导数，并使其等于 0，得到：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial f}{\partial \beta_0} &= -2\sum_{i=1}^n (Y_i - \beta_0 - \beta_1X)=0 \\
\frac{\partial f}{\partial \beta_1} &= -2 \sum_{i=1}^n X_i(Y_i - \beta_0 - \beta_1X_i)=0
\end{align}</script><p>对其进行求解，就可以得到下列参数方程。</p>
<!-- </details> -->
<p>一元线性回归参数方程：</p>
<script type="math/tex; mode=display">
\begin{align}
 &\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}\\
 &\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X} 
\end{align}</script><p>其中 $\bar{X}$和$\bar{Y}$为样本均值。<br>多元回归的推导更加复杂，但逻辑和上述回归一样，这里不再推导。</p>
<h3 id="假设条件"><a href="#假设条件" class="headerlink" title="假设条件"></a>假设条件</h3><h4 id="1-线性关系假定"><a href="#1-线性关系假定" class="headerlink" title="1.线性关系假定"></a>1.线性关系假定</h4><p>线性回归模型假定，$Y$与$X$之间的关系为参数线性关系，即$(3)$式，若形式为：</p>
<script type="math/tex; mode=display">\begin{align} Y=\beta_0+\beta_1X_1+ \beta_2X_2^2 + \varepsilon \end{align}</script><p>该模型仍然是线性关系，但若形式为：</p>
<script type="math/tex; mode=display">\begin{align} Y=\beta_0+\beta_1X_1+ \beta_2^2X_2 + \varepsilon \end{align}</script><p>则该模型不再满足线性关系，因为参数$\beta_2$并不是一次幂，于是参数就无法使用 OLS 估计了。简单来说，该假定强调了模型的参数必须是一次幂，而不限制变量的幂。例如：</p>
<script type="math/tex; mode=display">\begin{align}lnY=\beta_0+\beta_1lnX_1+ \beta_2lnX_2 + \varepsilon \end{align}</script><p>仍然是满足线性关系假定的，这种模型也被称为双对数模型，在实践中也经常被使用，因为该模型表达了解释变量$X$对被解释变量$Y$的边际效用。<br>例如经济学中经典的生产函数柯布-道格拉斯生产函数，其数学表达式为：</p>
<script type="math/tex; mode=display">\begin{align}Y=AK^\alpha L^\beta \end{align}</script><p>对两边取对数，可以得到：</p>
<script type="math/tex; mode=display">\begin{align}lnY=lnA+\alpha lnK+\beta lnL+ \varepsilon \end{align}</script><p>通过对数变换，就可以将非线性模型转换为线性模型。</p>
<h4 id="2-误差项零均值假定"><a href="#2-误差项零均值假定" class="headerlink" title="2.误差项零均值假定"></a>2.误差项零均值假定</h4><p>该假定义为：$E(\varepsilon) = 0$，从最小二乘法的推导过程中可以看到，我们的目标就是使得残差的平方和最小。在$(8)$式中，$\beta<em>0 = 2\sum</em>{i=1}^n (Y<em>i - \beta_0 - \beta_1X)=0$，其中其实就是让残差之和为零，又残差项均值为$\bar\varepsilon=\frac{\sum</em>{i}^n \varepsilon}{n}$，所以$\bar\varepsilon=0$。上面谈到$E(Y_i | X_i)=Y$，因此，可以得到$E(\varepsilon | X_i)=0$，即残差项的期望为零，又$E(\varepsilon)=E(E(\varepsilon | X_i))=E(0)=0$，因此，残差项的期望为零。在模型的构建中，也常用这个来检验模型的构建是否正确。<br>stata代码实操：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">sysuse</span> auto, <span class="keyword">clear</span> <span class="comment">// 导入 stata 自带数据</span></span><br><span class="line"><span class="keyword">reg</span> price weight      <span class="comment">// 线性回归：price = β0 + β1*weight + ε</span></span><br><span class="line"><span class="keyword">predict</span> y_hat, xb     <span class="comment">// 生成预测值 (ŷ = Xβ̂)</span></span><br><span class="line"><span class="keyword">gen</span> resid = price - y_hat <span class="comment">// 计算残差 (ε̂ = ŷ - y)</span></span><br><span class="line"><span class="keyword">sum</span> resid, detail     <span class="comment">// 残差描述性统计</span></span><br><span class="line"><span class="keyword">di</span> %9.2f <span class="built_in">r</span>(<span class="keyword">sum</span>)       <span class="comment">// 显示残差和（保留2位小数）</span></span><br><span class="line">     0.00  <span class="comment">// 这个地方就是残差和</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><br>可以从上面的回归结果发现，残差和正好为 0，这里保留 2 位小数是因为stata 在计算过程中，可能会因为精度的不同，结构会有些许误差。</p>
<h4 id="3-多重共线性假设"><a href="#3-多重共线性假设" class="headerlink" title="3.多重共线性假设"></a>3.多重共线性假设</h4><p>该假定假设当模型存在多个解释变量时即当模型是多元线性回归的时候，这些解释变量之间不存在完全的线性关系。这是因为当模型的多个变量存在完全的线性关系时，模型参数将无法估计。假定模型为：</p>
<script type="math/tex; mode=display">\begin{align}Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \varepsilon \end{align}$$$$\begin{align}X_2 = \alpha X_1 + \varepsilon \end{align}</script><p>则可以发现，$X_2$和$X_1$之间存在完全的线性关系，则$(15)$式可以变为：</p>
<script type="math/tex; mode=display">\begin{align}
Y &= \alpha_0 + \beta_1X_1 + \beta_2(\alpha X_1 + \varepsilon) + \varepsilon \\
  &= \alpha_0 + (\beta_1 + \alpha\beta_2)X_1 + (\beta_2 + 1)\varepsilon \\
  &= \alpha_0 + \beta X_1 + \varphi
\end{align}</script><p>其中$\beta = \beta_1 + \alpha\beta_2$， $\varphi = (\beta_2 + 1)\varepsilon$。<br>可以发现，$X_2$的存在是没有意义的，因为$X_2$对$Y$的贡献已经由$X_1$的贡献所代替。在 stata 中，若存在完全的多重共线性，模型会将其忽略掉，显示下面的结果：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">sysuse</span> auto, <span class="keyword">clear</span> # 载入数据</span><br><span class="line"><span class="keyword">rename</span> weight x1 # 重命名变量为x1</span><br><span class="line"><span class="keyword">gen</span> x2 = 1 + 2 * x1 # 生成变量x2，x2 = 1 + 2 * x1</span><br><span class="line"><span class="keyword">reg</span> price  x2 x1</span><br><span class="line"><span class="keyword">note</span>: x1 omitted because of collinearity.</span><br><span class="line"></span><br><span class="line">      Source |       SS           df       MS      Number of obs   =        74</span><br><span class="line">-------------+----------------------------------   <span class="built_in">F</span>(1, 72)        =     29.42</span><br><span class="line">       Model |   184233937         1   184233937   <span class="keyword">Prob</span> &gt; F        =    0.0000</span><br><span class="line">    Residual |   450831459        72  6261548.04   R-squared       =    0.2901</span><br><span class="line">-------------+----------------------------------   Adj R-squared   =    0.2802</span><br><span class="line">       <span class="keyword">Total</span> |   635065396        73  8699525.97   Root MSE        =    2502.3</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">       price | Coefficient  Std. <span class="keyword">err</span>.      t    P&gt;|t|     [95% <span class="keyword">conf</span>. interval]</span><br><span class="line">-------------+----------------------------------------------------------------</span><br><span class="line">          x2 |   1.022031   .1884171     5.42   0.000     .6464287    1.397634</span><br><span class="line">          x1 |          0  (omitted)</span><br><span class="line">       _cons |  -7.729385   1174.612    -0.01   0.995    -2349.276    2333.817</span><br><span class="line">------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure></p>
<p>可以看到，$X_2 =1 + 2*X_1$，这个是$X_1$变量是存在完全的多重共线性关系的，于是 Stata 会自动忽略掉该变量。而在其他的程序中，可能会出现错误，比如在 python 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> statsmodels.formula.api <span class="keyword">import</span> ols</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 导入 sklearn 数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 合并数据</span></span><br><span class="line">data = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis=<span class="number">1</span>)</span><br><span class="line">data.columns = [<span class="string">&quot;x1&quot;</span>, <span class="string">&quot;x2&quot;</span>, <span class="string">&quot;x3&quot;</span>, <span class="string">&quot;x4&quot;</span>, <span class="string">&quot;y&quot;</span>]</span><br><span class="line"><span class="comment"># 构造多重共线性数据</span></span><br><span class="line">data[<span class="string">&quot;x1&quot;</span>] = <span class="number">1</span> + <span class="number">2</span> * data[<span class="string">&quot;x2&quot;</span>]</span><br><span class="line"><span class="comment"># 模型拟合</span></span><br><span class="line">model = ols(<span class="string">&#x27;y ~ x1 + x2 + x3 + x4 &#x27;</span>, data).fit()</span><br><span class="line"><span class="built_in">print</span>(model.summary())</span><br><span class="line">                            OLS Regression Results</span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:                      y   R-squared:                       <span class="number">0.929</span></span><br><span class="line">Model:                            OLS   Adj. R-squared:                  <span class="number">0.927</span></span><br><span class="line">Method:                 Least Squares   F-statistic:                     <span class="number">632.8</span></span><br><span class="line">Date:                Thu, <span class="number">24</span> Jul <span class="number">2025</span>   Prob (F-statistic):           <span class="number">1.98e-83</span></span><br><span class="line">Time:                        <span class="number">11</span>:<span class="number">19</span>:<span class="number">36</span>   Log-Likelihood:                 <span class="number">15.513</span></span><br><span class="line">No. Observations:                 <span class="number">150</span>   AIC:                            -<span class="number">23.03</span></span><br><span class="line">Df Residuals:                     <span class="number">146</span>   BIC:                            -<span class="number">10.98</span></span><br><span class="line">Df Model:                           <span class="number">3</span></span><br><span class="line">Covariance <span class="type">Type</span>:            nonrobust</span><br><span class="line">==============================================================================</span><br><span class="line">                 coef    std err          t      P&gt;|t|      [<span class="number">0.025</span>      <span class="number">0.975</span>]</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">Intercept      <span class="number">0.0200</span>      <span class="number">0.162</span>      <span class="number">0.123</span>      <span class="number">0.902</span>      -<span class="number">0.300</span>       <span class="number">0.340</span></span><br><span class="line">x1            -<span class="number">0.0412</span>      <span class="number">0.015</span>     -<span class="number">2.701</span>      <span class="number">0.008</span>      -<span class="number">0.071</span>      -<span class="number">0.011</span></span><br><span class="line">x2            -<span class="number">0.0306</span>      <span class="number">0.074</span>     -<span class="number">0.414</span>      <span class="number">0.679</span>      -<span class="number">0.176</span>       <span class="number">0.115</span></span><br><span class="line">x3             <span class="number">0.1493</span>      <span class="number">0.040</span>      <span class="number">3.743</span>      <span class="number">0.000</span>       <span class="number">0.070</span>       <span class="number">0.228</span></span><br><span class="line">x4             <span class="number">0.6715</span>      <span class="number">0.090</span>      <span class="number">7.488</span>      <span class="number">0.000</span>       <span class="number">0.494</span>       <span class="number">0.849</span></span><br><span class="line">==============================================================================</span><br><span class="line">Omnibus:                        <span class="number">0.576</span>   Durbin-Watson:                   <span class="number">1.138</span></span><br><span class="line">Prob(Omnibus):                  <span class="number">0.750</span>   Jarque-Bera (JB):                <span class="number">0.299</span></span><br><span class="line">Skew:                           <span class="number">0.084</span>   Prob(JB):                        <span class="number">0.861</span></span><br><span class="line">Kurtosis:                       <span class="number">3.139</span>   Cond. No.                     <span class="number">1.74e+16</span></span><br><span class="line">==============================================================================</span><br><span class="line"></span><br><span class="line">Notes:</span><br><span class="line"><span class="comment"># 这儿报了多重共线性错误</span></span><br><span class="line">[<span class="number">1</span>] Standard Errors assume that the covariance matrix of the errors <span class="keyword">is</span> correctly specified.</span><br><span class="line">[<span class="number">2</span>] The smallest eigenvalue <span class="keyword">is</span> <span class="number">3.82e-29</span>. This might indicate that there are</span><br><span class="line">strong multicollinearity problems <span class="keyword">or</span> that the design matrix <span class="keyword">is</span> singular.</span><br></pre></td></tr></table></figure>
<p>可以发现在上面，也报出了多重共线性的错误。</p>
<h4 id="4-同方差假设"><a href="#4-同方差假设" class="headerlink" title="4.同方差假设"></a>4.同方差假设</h4><p>该假设说明了误差项的方差是是一个常数，即$Var(\varepsilon|X) = \sigma^2$，它不应该随误差项的改变而改变，这是为了确保普通最小二乘法（OLS）的标准误估计有效，从而保证假设检验（t 检验、F 检验）和置信区间的准确性。</p>
<h4 id="5-误差项无自相关"><a href="#5-误差项无自相关" class="headerlink" title="5.误差项无自相关"></a>5.误差项无自相关</h4><p>该假设说明了误差项之间不存在自相关关系，即$Cov(\varepsilon_i | \varepsilon_j) = 0, (i \neq j)$。简单来说，就是误差项应该是随机的，而不是具有相关性的。</p>
<h4 id="6-外生性假定"><a href="#6-外生性假定" class="headerlink" title="6.外生性假定"></a>6.外生性假定</h4><p>该假设是说，解释变量与误差项不存在相关关系，即$Cov(X_i,\varepsilon_i)=0$，进而$E(\varepsilon_i | X)=0$，排除遗漏变量偏差和双向因果关系，确保 OLS 估计量的一致性和无偏性。</p>
<h4 id="7-误差项正态性"><a href="#7-误差项正态性" class="headerlink" title="7.误差项正态性"></a>7.误差项正态性</h4><p>该假设是说，误差项是服从正态分布的，即$\varepsilon_i \sim N(0, \sigma^2)$。其中$\sigma^2$为误差项的方差。该假设由同方差假设和均值假设组成。</p>
<p>为了文章的简洁性，各种假设的检验方法，我将在后面的文章中详细介绍。</p>
<h3 id="最小二乘法统计性质"><a href="#最小二乘法统计性质" class="headerlink" title="最小二乘法统计性质"></a>最小二乘法统计性质</h3><p>之所以线性回归会使用最小二乘法$（OLS）$估计方法，是因为其具备良好的统计性质，主要包括线性性、无偏性、一致性，即在经典假设下，OLS是 <strong>最优线性无偏估计（BLUE）</strong>，也称高斯-马尔可夫定理。</p>
<ul>
<li>线性性<br>线性性是指利用 $OLS$ 估计出来的参数是观测者$Y$的线性组合。从式$（9）$就可以看出来。参数$\beta_i$都是$Y$的线性组合。<script type="math/tex; mode=display">
\begin{align}
&\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}\\
&\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X} 
\end{align}</script></li>
<li>无偏性<br>无偏性是指，参数的估计值$\hat\beta_i$的期望与参数的真实值$\beta_i$相同，即$E(\hat\beta_i) = \beta_i$<!-- 证明：
由假设条件可知，$E(\varepsilon_i) = 0$，
又：$\varepsilon_i = Y_i - \hat{Y}_i$，
$$
\begin{align}
\therefore  E(\varepsilon_i) &= E(Y_i - \hat{Y}_i) = 0\\
又\because Y_i &= \beta_0 + \beta_1X_i, \hat Y_i = \hat \beta_0 + \hat\beta_1X_i  \\
\therefore E(\varepsilon_i) &=E(Y_i) -E(\hat Y_i) =0\\
&=E(\beta_0 + \beta_1X_i) - E(\hat \beta_0 + \hat\beta_1X_i \\
&=E(\beta_0 + \beta_1X_i) = E(\hat \beta_0 + \hat\beta_1X_i) \\
&=E(\beta_0) + E(\beta_1X_i) =  E(\hat \beta_0) + E(\hat\beta_1X_i) \\
于是：&E(\hat \beta_0) = E(\beta_0)=\beta_0 \\
     &E(\hat \beta_1) = E(\beta_1)=\beta_1
\end{align}
$$ --></li>
<li>一致性<br>即 OLS 估计参数在所有的无偏线性估计里面，具备最小方差性。</li>
</ul>
<h3 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h3><p>符号规定：</p>
<ul>
<li>$n$: 样本数</li>
<li>$k$: 参数个数（不包括截距项）</li>
<li>$y_i$: 被解释变量真实值</li>
<li>$\hat y_i$: 被解释变量估计值</li>
<li>$\bar y$: 被解释变量的均值<h4 id="残差平方和-RSS"><a href="#残差平方和-RSS" class="headerlink" title="残差平方和$RSS$"></a>残差平方和$RSS$</h4>定义：<script type="math/tex; mode=display">
\begin{align} RSS = \sum_{i=1}^n(y_i - \hat y_i)^2\end{align}</script>含义：残差平方和，他表示了模型未能解释的部分，即估计的(\hat y<em>i) 与真实值(y_i) 之间的距离。<br>自由度：$df</em>{RSS} = n-k-1$<h4 id="回归平方和-ESS"><a href="#回归平方和-ESS" class="headerlink" title="回归平方和$ESS$"></a>回归平方和$ESS$</h4>定义：<script type="math/tex; mode=display">
\begin{align} ESS = \sum_{i=1}^n(\hat y_i - \bar y)^2\end{align}</script>含义：回归平方和，表示模型解释的部分。<br>自由度：$df_{ESS} = k$（含截距项模型）<h4 id="总平方和-TSS"><a href="#总平方和-TSS" class="headerlink" title="总平方和$TSS$"></a>总平方和$TSS$</h4>定义：<script type="math/tex; mode=display">
\begin{align} TSS &= \sum_{i=1}^n(y_i - \bar y)^2 \\
&= RSS + ESS
\end{align}</script>含义：总平方和，表示数据集的方差。<br>自由度：$df<em>{TSS} = df</em>{ESS} + df_{RSS} = n-1$    <!-- >注：本文中 RSS 表示残差平方和（Sum of Squared Residuals），ESS 表示回归平方和（Sum of Squares Explained）。有些教材中记法相反，此处以 residual 和 explained 命名为准。 -->
</li>
</ul>
<!-- #### 均方误差$MSE$
定义：
$$
\begin{align} MSE = \frac{ESS}{n-k-1}
\end{align}
$$

#### 回归均方$MSR$
定义：
$$
\begin{align} MSR &= \frac{RSS}{k} 
\end{align}
$$ -->
<h4 id="拟合优度-R-2"><a href="#拟合优度-R-2" class="headerlink" title="拟合优度$R^2$"></a>拟合优度$R^2$</h4><p>定义：</p>
<script type="math/tex; mode=display">
\begin{align}
R^2 &= 1 - \frac{\sum_{i=1}^n(y_i - \hat{y_i})^2}{\sum_{i=1}^n(y_i - \bar{y})^2} \\
       &= 1 - \frac{RSS}{TSS} \\
       &= \frac{ESS}{TSS}
\end{align}</script><p>其中，$R^2$为模型拟合程度，也被称作可决系数、拟合优度等。它表示回归模型对因变量波动的解释比例，即解释变量对因变量总变异的解释程度。。从这个角度也能看出，$R^2$的取值范围为$[0,1]$，越接近 1，说明模型解释程度越好，否则，模型解释程度越差。</p>
<blockquote>
<p>注:$R^2$越大不一定越好。因为增加新的解释变量通常会导致$R^2$增加，即使这些变量与因变量关系不大；</p>
<h4 id="调整后的拟合优度-bar-R-2"><a href="#调整后的拟合优度-bar-R-2" class="headerlink" title="调整后的拟合优度$\bar R^2$"></a>调整后的拟合优度$\bar R^2$</h4><p>定义：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\bar R^2 &= 1 - \frac{ESS/(n-k-1)}{TSS/(n-1)} \\
       &= 1 - \frac{MSE}{TSS/(n-1)} \\
         &= 1 - \frac{(1-R^2)(n-1)}{n-k-1}
\end{aligned}</script><p>$\bar R^2$是调整后的可决系数，这是因为拟合优度$R^2$计算存在一些缺陷，$R^2$会受到模型解释变量的个数的影响，也就是说，只要通过增加解释变量的个数，就可以在一定程度上使得$R^2$增加。但是<a href="#3多重共线性假设">假设3</a>提到，模型的解释变量之间不能严重的存在多重共线性，因此，$\bar R^2$就增加了一个对$R^2$的修正，使得$\bar R^2$在增加解释变量个数时，会对$R^2$施加一定的惩罚系数。因此使用$\bar R^2$有助于一定程度上抑制无效变量的引入，增强模型的稳健性和可解释性。当新加入变量没有带来模型提升时，$\bar R^2$ 可能下降，防止过拟合。</p>
<p>注：选择使用哪个系数进行判断，需要根据实际情况选择。</p>
<h3 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h3><p>这里的假设检验并不是对各种假设条件的检验，而是对估计参数的统计检验，也就是通过统计学的方法来判断我们估计的参数是否有效，主要包括了 $t$ 检验和 $F$ 检验。</p>
<h4 id="t检验"><a href="#t检验" class="headerlink" title="t检验"></a>t检验</h4><p>设原假设为：</p>
<script type="math/tex; mode=display">H_0: \beta_i = 0</script><p>备择假设（双尾检验）为：</p>
<script type="math/tex; mode=display">H_1: \beta_i \neq 0</script><p>注：若理论支持方向性，可改用单尾检验（如$H_1: \beta_i &gt; 0$或$H_1: \beta_i &lt; 0$）</p>
</blockquote>
<p>即原假设假设解释变量对被解释变量不具备影响能力。备择假设则认为解释变量对被解释变量有影响。<br>其实可以发现，原假设和备择假设是对立的，在 t 检验中，我们假设原假设为真，然后通过构造t 统计量来判断，如果 t 统计量大于 t 值，则原假设被拒绝，否则原假设被接受。t 统计量计算公式为：</p>
<script type="math/tex; mode=display">\begin{align}t(n-k-1) = \frac{\hat{\beta_i} - \beta_i}{SE(\hat{\beta_i})}\end{align}</script><p>其中，$n-k-1$是自由度，$n、k$分别为样本量和解释变量个数，$\hat{\beta_i}$是系数的估计值，$\beta_i$为原假设的理论值，$SE(\hat{\beta_i})$是系数的估计标准差。在这里，原假设是$\beta_i = 0$，于是上式就可以变为：</p>
<script type="math/tex; mode=display">\begin{align}t(n-k-1) = \frac{\hat{\beta_i} - 0}{SE(\hat{\beta_i})}\end{align}</script><p>得到 t 统计量后，我们首先要查表确定在这个自由度下的$t<em>{\alpha/2}{(n-k-1)}$的临界值，例如，95%的显著性水平的双尾检验的$\alpha$ = 0.0），$t</em>{\alpha/2}$（的临界值为2.262。<br>若我们计算得到的$｜t｜ &gt; t_{\alpha/2}$，则认为拒绝原假设，选择备择假设。否则，不能拒绝原假设。<br>或者使用 $p$值，查找计算出的 $t$ 统计量所对应的$p$值，若$p&lt; \alpha$，则拒绝原假设，否则不能拒绝原假设。<br>在 stata 中可以通过以下命令实现：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">. <span class="keyword">display</span> <span class="built_in">invttail</span>(20, 0.025) #其中20为自由度，0.025为α， 得到的是临界值</span><br><span class="line">2.0859634</span><br><span class="line"></span><br><span class="line">. <span class="keyword">display</span> 2 * <span class="built_in">ttail</span>(20, <span class="built_in">abs</span>(4)) #其中 20 为自由度，4为 t 统计量，（若是双尾检验则需要*2，若是单尾则不需要），得到p值</span><br><span class="line">.00070352</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>这一部分可以看<a href="#stata代码实现">代码实现</a>部分，会没那么抽象。</p>
<h4 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a>F检验</h4><p>F 检验主要用于检验多个解释变量之间是否同时满足某个条件，例如模型设为：</p>
<script type="math/tex; mode=display">Y = \beta_0 + \beta_1X_1 + \beta_2X_2 +\dots+ \beta_nX_n + \varepsilon</script><p>则 F 检验的原假设为：</p>
<script type="math/tex; mode=display">H_0: \beta_1 = \beta_2 = \dots =\beta_n = 0</script><p>备择假设为：</p>
<script type="math/tex; mode=display">H_1: 至少一个\beta_i \neq 0， \{i=1,2,\dots,n\}</script><p>F 检验的统计量计算公式为：</p>
<script type="math/tex; mode=display">\begin{align} F(k-1, n-k) &&=\frac{ESS/(k-1)}{RSS/(n-k)} \end{align}</script><p>其中：</p>
<ul>
<li>$RSS$为残差平方和；</li>
<li>$ESS$为回归平方和；</li>
<li>$k$为解释变量个数，即解释变量个数；</li>
<li>$n$为样本个数，即样本个数。</li>
</ul>
<p>F 检验实际上是在检验整个模型的有效性，若解释变量的系数均为 0，那么这个模型也就无效了，若起码有一个系数不为 0，那么这个模型仍然是有效的。这是一个很关键的点，</p>
<h3 id="stata代码实现"><a href="#stata代码实现" class="headerlink" title="stata代码实现"></a>stata代码实现</h3><p>我们使用 stata 自带的 auto 数据进行一次一元的线性回归分析，以 price为被解释变量，weight 为解释变量，设模型为如下形式：</p>
<script type="math/tex; mode=display">Y = \beta_0 + \beta_1X + \varepsilon</script><p>其中$Y$为 price，$X$为 weight，$\varepsilon$为误差项。<br>首先打开 stata ，输入以下代码：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">. <span class="keyword">sysuse</span> auto, <span class="keyword">clear</span> <span class="comment">// 导入 stata 自带数据</span></span><br><span class="line">(1978 automobile data)</span><br><span class="line"></span><br><span class="line">. </span><br><span class="line">end of <span class="keyword">do</span>-<span class="keyword">file</span></span><br></pre></td></tr></table></figure><br>这一步操作主要是导入数据 stata 自带的 auto 数据集，<br>接下来，输入以下代码：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">. <span class="keyword">rename</span> price Y <span class="comment">// 我们将price重命名为 Y（这一步非必需，只是为了符号统一）</span></span><br><span class="line"></span><br><span class="line">. <span class="keyword">rename</span> weight X <span class="comment">// 我们将weight重命名为 X（这一步非必需，只是为了符号统一）</span></span><br><span class="line"></span><br><span class="line">. <span class="keyword">reg</span> Y X      <span class="comment">// 这一步是拟合模型，Y = β0 + β1*X + ε</span></span><br><span class="line"></span><br><span class="line">      Source |       SS           df       MS      Number of obs   =        74</span><br><span class="line">-------------+----------------------------------   <span class="built_in">F</span>(1, 72)        =     29.42</span><br><span class="line">       Model |   184233937         1   184233937   <span class="keyword">Prob</span> &gt; F        =    0.0000</span><br><span class="line">    Residual |   450831459        72  6261548.04   R-squared       =    0.2901</span><br><span class="line">-------------+----------------------------------   Adj R-squared   =    0.2802</span><br><span class="line">       <span class="keyword">Total</span> |   635065396        73  8699525.97   Root MSE        =    2502.3</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">           Y | Coefficient  Std. <span class="keyword">err</span>.      t    P&gt;|t|     [95% <span class="keyword">conf</span>. interval]</span><br><span class="line">-------------+----------------------------------------------------------------</span><br><span class="line">           X |   2.044063   .3768341     5.42   0.000     1.292857    2.795268</span><br><span class="line">       _cons |  -6.707353    1174.43    -0.01   0.995     -2347.89    2334.475</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">. </span><br><span class="line">end of <span class="keyword">do</span>-<span class="keyword">file</span></span><br></pre></td></tr></table></figure><br>结果如上所示，现在我们来解析一下这个结果，首先看左上方的结果，如下所示，其中$Source$ 表示来源，$SS$ 为平方和，$df$ 表示自由度，$MS $表示均方差。第二行第三行分别表示模型和残差$SS$、$df$、$MS$，以及总$SS$、$df$、$MS$。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">      Source |       SS           df       MS      </span><br><span class="line">-------------+----------------------------------   </span><br><span class="line">       Model |   184233937         1   184233937   </span><br><span class="line">    Residual |   450831459        72  6261548.04   </span><br><span class="line">-------------+----------------------------------   </span><br><span class="line">       <span class="keyword">Total</span> |   635065396        73  8699525.97  </span><br></pre></td></tr></table></figure><br>然后再看右上方的结果，如下所示，其中$F(1, 72)=29.42$表示自由度为$(1, 72)$的$F$检验统计值为$29.42$，$P&gt;F$ 表示$F$检验的$p$值，R-squared表示$R^2$值，即拟合优度（决定系数），Adj R-squared表示调整后的$R^2$值，Root MSE 表示根均方误差（回归标准误）。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Number of obs   =        74</span><br><span class="line"><span class="built_in">F</span>(1, 72)        =     29.42</span><br><span class="line"><span class="keyword">Prob</span> &gt; F        =    0.0000</span><br><span class="line">R-squared       =    0.2901</span><br><span class="line">Adj R-squared   =    0.2802</span><br><span class="line">Root MSE        =    2502.3</span><br></pre></td></tr></table></figure><br>最后再来看下方的参数估计表，如下方所示，其中第一行为标头，分别表示被解释变量$Y$，Coefficient为解释变量的系数$\beta$的估计值，Std. err为$\beta$的估计值的标准误差，$t$为$\beta$的估计值的t统计量，$P&gt;|t|$为$t$统计量的$p$值，[0.025, 0.975]为95%的置信区间其中，$X$行表示X的所有值，_cons为常数项。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------</span><br><span class="line">           Y | Coefficient  Std. <span class="keyword">err</span>.      t    P&gt;|t|     [95% <span class="keyword">conf</span>. interval]</span><br><span class="line">-------------+----------------------------------------------------------------</span><br><span class="line">           X |   2.044063   .3768341     5.42   0.000     1.292857    2.795268</span><br><span class="line">       _cons |  -6.707353    1174.43    -0.01   0.995     -2347.89    2334.475</span><br><span class="line">------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><br>于是通过上面的分析，我们可以写出如下的估计线性方程：</p>
<script type="math/tex; mode=display">\begin{align} Y = -6.707353 + 2.044063X + \varepsilon \end{align}</script><h3 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h3><p>我们通过使用 sklearn 的鸾尾花数据集进行分析，得到下面的结果，解读方法和stata 的差不多，只是在这儿，常数是Intercept，coef 是系数，std err 是标准误差，Prob (F-statistic)是 F 检验的p值。更多的介绍将在后面分析。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> statsmodels.formula.api <span class="keyword">import</span> ols</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 导入 sklearn 数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 合并数据</span></span><br><span class="line">data = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis=<span class="number">1</span>)</span><br><span class="line">data.columns = [<span class="string">&quot;x1&quot;</span>, <span class="string">&quot;x2&quot;</span>, <span class="string">&quot;x3&quot;</span>, <span class="string">&quot;x4&quot;</span>, <span class="string">&quot;y&quot;</span>]</span><br><span class="line"><span class="comment"># 构造多重共线性数据</span></span><br><span class="line"><span class="comment"># 模型拟合</span></span><br><span class="line">model = ols(<span class="string">&#x27;y ~ x1 + x2 + x3 + x4 &#x27;</span>, data).fit()</span><br><span class="line"><span class="built_in">print</span>(model.summary()) </span><br><span class="line">                            OLS Regression Results                            </span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:                      y   R-squared:                       <span class="number">0.930</span></span><br><span class="line">Model:                            OLS   Adj. R-squared:                  <span class="number">0.928</span></span><br><span class="line">Method:                 Least Squares   F-statistic:                     <span class="number">484.5</span></span><br><span class="line">Date:                Sat, <span class="number">26</span> Jul <span class="number">2025</span>   Prob (F-statistic):           <span class="number">8.46e-83</span></span><br><span class="line">Time:                        <span class="number">14</span>:<span class="number">39</span>:<span class="number">31</span>   Log-Likelihood:                 <span class="number">17.437</span></span><br><span class="line">No. Observations:                 <span class="number">150</span>   AIC:                            -<span class="number">24.87</span></span><br><span class="line">Df Residuals:                     <span class="number">145</span>   BIC:                            -<span class="number">9.821</span></span><br><span class="line">Df Model:                           <span class="number">4</span>                                         </span><br><span class="line">Covariance <span class="type">Type</span>:            nonrobust                                         </span><br><span class="line">==============================================================================</span><br><span class="line">                 coef    std err          t      P&gt;|t|      [<span class="number">0.025</span>      <span class="number">0.975</span>]</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">Intercept      <span class="number">0.1865</span>      <span class="number">0.205</span>      <span class="number">0.910</span>      <span class="number">0.364</span>      -<span class="number">0.218</span>       <span class="number">0.591</span></span><br><span class="line">x1            -<span class="number">0.1119</span>      <span class="number">0.058</span>     -<span class="number">1.941</span>      <span class="number">0.054</span>      -<span class="number">0.226</span>       <span class="number">0.002</span></span><br><span class="line">x2            -<span class="number">0.0401</span>      <span class="number">0.060</span>     -<span class="number">0.671</span>      <span class="number">0.503</span>      -<span class="number">0.158</span>       <span class="number">0.078</span></span><br><span class="line">x3             <span class="number">0.2286</span>      <span class="number">0.057</span>      <span class="number">4.022</span>      <span class="number">0.000</span>       <span class="number">0.116</span>       <span class="number">0.341</span></span><br><span class="line">x4             <span class="number">0.6093</span>      <span class="number">0.094</span>      <span class="number">6.450</span>      <span class="number">0.000</span>       <span class="number">0.423</span>       <span class="number">0.796</span></span><br><span class="line">==============================================================================</span><br><span class="line">Omnibus:                        <span class="number">0.374</span>   Durbin-Watson:                   <span class="number">1.077</span></span><br><span class="line">Prob(Omnibus):                  <span class="number">0.829</span>   Jarque-Bera (JB):                <span class="number">0.141</span></span><br><span class="line">Skew:                          -<span class="number">0.051</span>   Prob(JB):                        <span class="number">0.932</span></span><br><span class="line">Kurtosis:                       <span class="number">3.110</span>   Cond. No.                         <span class="number">91.9</span></span><br><span class="line">==============================================================================</span><br><span class="line"></span><br><span class="line">Notes:</span><br><span class="line">[<span class="number">1</span>] Standard Errors assume that the covariance matrix of the errors <span class="keyword">is</span> correctly specified.</span><br></pre></td></tr></table></figure></p>
<p>参考文献：<br>[1]. <a target="_blank" rel="noopener" href="https://zh.wikipedia.org/w/index.php?title=%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8&amp;oldid=88113585.">维基百科编者. 線性回歸[G/OL]. 维基百科, 2025(20250705)[2025-07-05].</a><br>[2]. Wooldridge, J. M. (2010). Econometric Analysis of Cross Section and Panel Data.<br>[3]. Greene, W. H. (2012). Econometric Analysis.</p>
</div><script src="https://cdn.jsdelivr.net/lightgallery.js/1.0.1/js/lightgallery.min.js"></script><script src="https://gitee.com/mirrors_sachinchoolur/lg-pager.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script>
    </div>
  </div>

  
    
  <div class="sea-prev-next-wrapper">
    
      <div class="prev">
        <svg t="1725418977480" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4239" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M384 512L731.733333 202.666667c17.066667-14.933333 19.2-42.666667 4.266667-59.733334-14.933333-17.066667-42.666667-19.2-59.733333-4.266666l-384 341.333333c-10.666667 8.533333-14.933333 19.2-14.933334 32s4.266667 23.466667 14.933334 32l384 341.333333c8.533333 6.4 19.2 10.666667 27.733333 10.666667 12.8 0 23.466667-4.266667 32-14.933333 14.933333-17.066667 14.933333-44.8-4.266667-59.733334L384 512z" p-id="4240"></path></svg>
        <a class="link" href="/2025/cmdv0aw3h000utgs6hw5lb43o/">
          计量｜什么是虚拟变量
        </a>
      </div>
    
    
      <div class="next">
        <a class="link" href="/2025/cmdv0aw4h005ctgs60c9fe23d/">
          诗歌 低吟的尽头
        </a>
        <svg t="1725418993065" class="sea-svg-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6832" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M731.733333 480l-384-341.333333c-17.066667-14.933333-44.8-14.933333-59.733333 4.266666-14.933333 17.066667-14.933333 44.8 4.266667 59.733334L640 512 292.266667 821.333333c-17.066667 14.933333-19.2 42.666667-4.266667 59.733334 8.533333 8.533333 19.2 14.933333 32 14.933333 10.666667 0 19.2-4.266667 27.733333-10.666667l384-341.333333c8.533333-8.533333 14.933333-19.2 14.933334-32s-4.266667-23.466667-14.933334-32z" p-id="6833"></path></svg>
      </div>
    
  </div>

  
</article>



<script defer>
  document.addEventListener('DOMContentLoaded', function () {
    const toggleIcon = document.querySelector('.sea-article-catalog-title .sea-svg-icon');
    const tocContent = document.querySelector('.sea-article-catalog > .toc');
    if (toggleIcon && tocContent) {
      toggleIcon.addEventListener('click', function () {
        tocContent.classList.toggle('sea-article-catalog-show');
        toggleIcon.classList.toggle('sea-svg-icon-rotate');
      });
    }
  });
</script>
  </main>
  <footer id="sea-footer-container">
  <div class="sea-footer-row">
    <div class="sea-footer-menu-link">
      
        <a
          class="sea-footer-link"
          
          href="/joakimstarr@qq.com"
        >
          Email
        </a>
        <span class="sea-footer-link__dot">·</span>
      
        <a
          class="sea-footer-link"
          
            target="_blank"
          
          href="https://github.com/joakimstarr"
        >
          Github
        </a>
        <span class="sea-footer-link__dot">·</span>
      
        <a
          class="sea-footer-link"
          
            target="_blank"
          
          href="https://www.zhihu.com/people/miskies"
        >
          Zhihu
        </a>
        <span class="sea-footer-link__dot">·</span>
      
    </div>
  </div>
  
  
  <div class="sea-footer-row">
    <div class="sea-footer-copyright">
      <span>©</span>
      
        2025
      
      <span>·</span>
      Joakim Starr(文人病)
    </div>
    <span class="split-line">|</span>
    <div class="sea-footer-theme-by">
      Theme by <a class="theme" href="https://github.com/hai-zou/hexo-theme-sea" target="_blank">Sea</a>
    </div>
  </div>
</footer>

  
<script src="/js/main.js" defer></script>


<script src="/js/theme.js" defer></script>

</body>
</html>